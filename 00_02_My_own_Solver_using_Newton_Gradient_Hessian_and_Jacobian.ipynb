{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First and Second derivatives\n",
    "1-dim function, f(x) : Newton Solve and Optimize\n",
    "\n",
    "## Gradient, Hessian and Jacobian\n",
    "N-dim scalar function, f(x,y,z) and vector function, f(f0(x,y),f1(x,y)): Newton Solve and Optimize \n",
    "\n",
    "for the InverseJacobian\n",
    "http://fourier.eng.hmc.edu/e176/lectures/NM/node21.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of ideas of this notebook are from\n",
    "\n",
    "Massimo Di Pierro \n",
    "\n",
    "Annotated Algorithms in Python \n",
    "\n",
    "https://mdipierro.github.io/DePaul/CSC402/csc402-notes.pdf\n",
    "\n",
    "Chapter 4\n",
    "\n",
    "I modified the code to create an independent python notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import linalg as LA\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One dimension: Solver and Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0\n",
      "-5.000000000254801\n",
      "-5.000000000254801\n",
      "2.001065979584382\n",
      "2.000399845769607\n"
     ]
    }
   ],
   "source": [
    "# We are preparing 1st and second numerical derivatives for a one dimension f(x)\n",
    "def D(f , h=1e-6): # first derivative of f\n",
    "    return lambda x,f=f,h=h: (f(x+h)-f(x-h))/2.0/h\n",
    "\n",
    "def DD(f, h=1e-6): # second derivative of f\n",
    "    return lambda x,f=f,h=h: (f(x+h)-2.0*f(x)+f(x-h))/(h*h)\n",
    "\n",
    "def f(x): \n",
    "    return (x-2)*(x-5) \n",
    "\n",
    "x = 1.0\n",
    "print (f(x)) \n",
    "f1 = D(f)  # first derivative \n",
    "print (f1(x)) \n",
    "print (D(f)(x)) \n",
    "f2 = DD(f)  # second derivative \n",
    "print (DD(f)(x)) \n",
    "ff2 = D(f)  # second derivative \n",
    "print (D(ff2)(x)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solve_newton will allow us to find that point x0 where f(x0) is 0!\n",
    "def solve_newton(f, x, ap=1e-6, rp=1e-4, ns=20):\n",
    "    x = float(x) # make sure it is not int\n",
    "    for k in range(ns):\n",
    "        (fx, Dfx) = (f(x), D(f)(x))\n",
    "        if LA.norm(Dfx) < ap:\n",
    "            raise ArithmeticError('unstable solution')\n",
    "        (x_old, x) = (x, x-fx/Dfx)\n",
    "        if (k>2) & (LA.norm(x-x_old) < max(float(ap),LA.norm(x)*rp)): return x\n",
    "    raise ArithmeticError('no convergence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9999999993015118\n",
      "2.0954644690978615e-09\n",
      "2.0\n"
     ]
    }
   ],
   "source": [
    "sol = solve_newton(f,1.0)\n",
    "print (sol)\n",
    "# At the solution point f(x0) is zero!\n",
    "print (f(sol))\n",
    "print (round(solve_newton(f,1.0),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimize_newton will allow us to find that point x0 where f(x0) is at the minimum (it might be a local minimum)\n",
    "def optimize_newton(f, x, ap=1e-6, rp=1e-4, ns=20): \n",
    "    x = float(x) # make sure it is not int \n",
    "    # **************************************************\n",
    "    # the following line is NOT in solve_netwon    \n",
    "    (f, Df) = (D(f), DD(f)) # with the optimizer, I set f = D(f) and D(f) = DD(f)\n",
    "    # **************************************************\n",
    "    for k in range(ns): \n",
    "        (fx, Dfx) = (f(x), Df(x)) \n",
    "        if Dfx==0: return x \n",
    "        if LA.norm(Dfx) < ap: \n",
    "            raise ArithmeticError('unstable solution') \n",
    "        (x_old, x) = (x, x-fx/Dfx) \n",
    "        if LA.norm(x-x_old) < max(ap,LA.norm(x)*rp): return x \n",
    "    raise ArithmeticError('no convergence')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0\n",
      "-5.000000000254801\n",
      "2.001065979584382\n",
      " testing : ------------\n",
      " The solution point, x0, where the function f() finds its minimum is :\n",
      "3.5000000000327596\n",
      " At the solution point, the function f() is :\n",
      "-2.25\n",
      " Let's check the gradient at the solution point. It should be 0\n",
      "0.0\n",
      " as the second derivative is > 0 at the solution point x0, the solution is a minimum \n",
      "2.000177801164682\n"
     ]
    }
   ],
   "source": [
    "x = 1.0\n",
    "print (f(x)) \n",
    "print (D(f)(x)) \n",
    "print (DD(f)(x)) \n",
    "print (\" testing : ------------\")\n",
    "solOp = optimize_newton(f,1.0)\n",
    "print (\" The solution point, x0, where the function f() finds its minimum is :\")\n",
    "print (solOp)\n",
    "print (\" At the solution point, the function f() is :\")\n",
    "print (f(solOp))\n",
    "print (\" Let's check the gradient at the solution point. It should be 0\")\n",
    "print (D(f)(solOp))\n",
    "\n",
    "print (\" as the second derivative is > 0 at the solution point x0, the solution is a minimum \")\n",
    "print (DD(f)(solOp)) "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "as the second derivative is > 0 at the solution point x0, the solution is a minimum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two dimension: Solver and Optimization\n",
    "\n",
    "We need, Gradient, Hessian and Jacobian (for a vector function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scalar function of three variables, x, y and z\n",
    "def f(x):\n",
    "    return 2.0*x[0]+3.0*x[1]+5.0*x[1]*x[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial(f,i,h=1e-4):\n",
    "    def df(x,f=f,i=i,h=h):\n",
    "        x = list(x) # make copy of x\n",
    "        x[i] += h\n",
    "        f_plus = f(x)\n",
    "        x[i] -= 2*h\n",
    "        f_minus = f(x)\n",
    "        if isinstance(f_plus,(list,tuple)):\n",
    "            return [(f_plus[i]-f_minus[i])/(2*h) for i in range(len(f_plus))]\n",
    "        else:\n",
    "            return (f_plus-f_minus)/(2*h)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.0\n"
     ]
    }
   ],
   "source": [
    "x = (1,1,1) \n",
    "print (round(f(x),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9999999999953388\n",
      "7.999999999999119\n",
      "4.999999999988347\n"
     ]
    }
   ],
   "source": [
    "df0 = partial(f,0) \n",
    "df1 = partial(f,1) \n",
    "df2 = partial(f,2) \n",
    "\n",
    "print (df0(x))\n",
    "print (df1(x))\n",
    "print (df2(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9999999999953388\n",
      "7.999999999999119\n",
      "4.999999999988347\n"
     ]
    }
   ],
   "source": [
    "print (partial(f,0)(x))\n",
    "print (partial(f,1)(x))\n",
    "print (partial(f,2)(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 8. 5.]\n",
      "[2. 8. 5.]\n",
      "[[0.         0.         0.        ]\n",
      " [0.         0.         5.00000006]\n",
      " [0.         5.00000006 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "def gradient(f, x, h=1e-4):\n",
    "    v = np.zeros(len(x))\n",
    "    for i in range(len(x)):\n",
    "        v[i] = partial(f,i,h)(x)\n",
    "    return v\n",
    "grad = gradient(f,x)\n",
    "print (grad)\n",
    "\n",
    "def gradient2(f, x, h=1e-4):\n",
    "    v = np.zeros(len(x))\n",
    "    for i in range(len(x)):\n",
    "        df   = partial(f,i,h) \n",
    "        v[i] = df(x)\n",
    "    return v\n",
    "grad2 = gradient2(f,x)\n",
    "print (grad2)\n",
    "\n",
    "def hessian(f, x, h=1e-4):\n",
    "    s = (len(x),len(x))\n",
    "    He = np.zeros(s) \n",
    "    for i in range(len(x)):\n",
    "        for j in range(len(x)):\n",
    "            He[j,i] = partial(partial(f,j,h),i,h)(x)              \n",
    "    return He\n",
    "Hess = hessian(f,x)\n",
    "print (Hess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector function of three variables, x, y and z\n",
    "def f2(x): \n",
    "    return (2.0*x[0]+3.0*x[1]+5.0*x[1]*x[2], 2.0*x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Testing : -------------------- \n",
      "(10.0, 2.0)\n",
      "[1.9999999999953388, 1.9999999999997797]\n",
      "[7.999999999999119, 0.0]\n",
      "[4.999999999988347, 0.0]\n"
     ]
    }
   ],
   "source": [
    "x = (1,1,1) \n",
    "print (\" Testing : -------------------- \")\n",
    "print (f2(x))\n",
    "df0 = partial(f2,0) \n",
    "df1 = partial(f2,1) \n",
    "df2 = partial(f2,2) \n",
    "print (df0(x))\n",
    "print (df1(x))\n",
    "print (df2(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Jacobian Calls : -------------------- \n",
      "[[2. 8. 5.]\n",
      " [2. 0. 0.]]\n",
      "<class 'numpy.ndarray'>\n",
      "[[2. 2.]\n",
      " [8. 0.]\n",
      " [5. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print (\" Jacobian Calls : -------------------- \")\n",
    "\n",
    "def jacobian(f, x, h=1e-4):\n",
    "    nof = len(f(x))\n",
    "    nox = len(x)\n",
    "    s = (nof,nox)\n",
    "    Jac = np.zeros(s) \n",
    "    for i in range(nox):     #colonne\n",
    "        for j in range(nof): # rows\n",
    "            ff = partial(f,i,h)(x)\n",
    "            Jac[j,i] = ff[j]              \n",
    "    return Jac\n",
    "Jac = jacobian(f2,x)\n",
    "print (Jac)\n",
    "print (type(Jac))\n",
    "print (Jac.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_newton_multi_Vector_F(f, x, ap=1e-6, rp=1e-4, ns=20): \n",
    "    \"\"\"\n",
    "    Computes the root of a multidimensional function f near point x.\n",
    "\n",
    "    Parameters\n",
    "    f is a function that takes a list and returns a scalar\n",
    "    x is a list\n",
    "\n",
    "    Returns x, solution of f(x)=0, as a list\n",
    "    \"\"\"\n",
    "    for k in range(ns): \n",
    "        (fx, Dfx) = (f(x), jacobian(f,x)) \n",
    "        if LA.norm(Dfx) < ap: \n",
    "            raise ArithmeticError('unstable solution')\n",
    "        Dfx = np.transpose(Dfx)\n",
    "        # prepare the jacobian pseudo inverse\n",
    "        Trans = np.transpose(Dfx)\n",
    "        Pseudo = np.dot(np.linalg.inv(np.dot(Trans, Dfx)), Trans)\n",
    "        (x_old, x) = (x, x-np.dot(fx,Pseudo)) \n",
    "        if LA.norm(x-x_old) < max(ap,LA.norm(x)*rp): return x \n",
    "    raise ArithmeticError('no convergence')\n",
    "\n",
    "def optimize_newton_multi_Scalar_F(f, x, ap=1e-6, rp=1e-4, ns=20): \n",
    "    for k in range(ns): \n",
    "        (fx, Dfx) = (gradient(f,x), hessian(f,x)) \n",
    "        if LA.norm(Dfx) < ap: \n",
    "            raise ArithmeticError('unstable solution')  \n",
    "        (x_old, x) = (x, x-np.dot(fx,np.linalg.inv(Dfx))) \n",
    "        if LA.norm(x-x_old) < max(ap,LA.norm(x)*rp): return x \n",
    "    raise ArithmeticError('no convergence')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is for a function returning a 2-dim vector, from R3 to R2. For this function we want to find the point where it returns the zero vector<0,0>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at x = <1,1,1>, f(x) is equal to: \n",
      "(10.0, 2.0)\n"
     ]
    }
   ],
   "source": [
    "# Remember from R3 to R2\n",
    "x = (1,1,1) \n",
    "print (\"at x = <1,1,1>, f(x) is equal to: \")\n",
    "print (f2(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "the point <x, y, z> where f(x,y,z) is = <0, 0> are: \n",
      "[0.     0.     0.4856]\n",
      "(1.7242216261363584e-16, 0.0)\n"
     ]
    }
   ],
   "source": [
    "print(\"--------------------\")\n",
    "# solve on a vector - function means to find the point x,y,z where the vector f(x,y,z) returns the zero vector <0,0>!\n",
    "sol = solve_newton_multi_Vector_F(f2, x=x)\n",
    "print (\"the point <x, y, z> where f(x,y,z) is = <0, 0> are: \")\n",
    "print (np.round(sol,4))\n",
    "print (f2(sol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "At the solution points, the Jacobian is NOT a zero vector/matrix! Remember this is a solve, not optimization. Let's check\n",
      "[[2.     5.4281 0.    ]\n",
      " [2.     0.     0.    ]]\n",
      "--------------------\n",
      "and finally, at the solution point <x, y, z> where f(x,y,z) is = <0, 0>: \n",
      "[0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(\"--------------------\")\n",
    "print (\"At the solution points, the Jacobian is NOT a zero vector/matrix! Remember this is a solve, not optimization. \\\n",
    "Let's check\")\n",
    "print (np.round(jacobian(f2,sol),4))\n",
    "print(\"--------------------\")\n",
    "print (\"and finally, at the solution point <x, y, z> where f(x,y,z) is = <0, 0>: \")\n",
    "print (np.round(f2(sol),4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is for a function returning a scalar, from R2 to R1. For this function we want to find the point where it is at its minimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the Gradient of f() at the initial point <5, 5> is:  [6. 4.]\n",
      "the Hessian Matrix of f() at the same point <5, 5> is: \n",
      "[[1.99999999 0.        ]\n",
      " [0.         1.99999999]]\n"
     ]
    }
   ],
   "source": [
    "def f(x): \n",
    "    return (x[0]-2)**2+(x[1]-3)**2 \n",
    "x = (5,5) \n",
    "\n",
    "grad = gradient(f,x)\n",
    "print (\"the Gradient of f() at the initial point <5, 5> is: \",grad)\n",
    "\n",
    "Hess = hessian(f,x)\n",
    "print (\"the Hessian Matrix of f() at the same point <5, 5> is: \") \n",
    "print (Hess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does the Hessian Matrix have an inverse ? \n",
      "[[0.5 0. ]\n",
      " [0.  0.5]]\n"
     ]
    }
   ],
   "source": [
    "print (\"Does the Hessian Matrix have an inverse ? \") \n",
    "print (np.linalg.inv(Hess))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "solution is  [2. 3.]\n",
      "f at solution is  4.930380657631324e-32\n",
      "grad at solution is \n",
      "[-4.44088455e-16  4.44088455e-16]\n",
      "The gradient is a zero vector, so this is a solution\n"
     ]
    }
   ],
   "source": [
    "print(\"--------------------\")\n",
    "sol = optimize_newton_multi_Scalar_F(f, x=x)\n",
    "print (\"solution is \",sol)\n",
    "print (\"f at solution is \",f(sol))\n",
    "print (\"grad at solution is \")\n",
    "print (gradient(f,sol))\n",
    "print (\"The gradient is a zero vector, so this is a solution\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------\n",
      "Let's look at the Hessian \n",
      "hessian at solution is :\n",
      "[[ 2.00000000e+00 -8.27180613e-17]\n",
      " [-8.27180613e-17  2.00000000e+00]]\n",
      "Is it a minumum? \n",
      "[[ 2.00000000e+00 -8.27180613e-17]\n",
      " [-8.27180613e-17  2.00000000e+00]]\n",
      "----------------------------------------------------- \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (\"-----------------------------------------------------\")\n",
    "print (\"Let's look at the Hessian \")\n",
    "print (\"hessian at solution is :\")\n",
    "print(hessian(f,sol))\n",
    "print (\"Is it a minumum? \")\n",
    "HessSol = hessian(f,sol)\n",
    "print(HessSol)\n",
    "print (\"----------------------------------------------------- \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To check, let's create a vector v, which when multiplied by H should return a non-zero vector, if the point is a minimum\n",
      "T(v) * H * v >0 \n",
      "10.000000000000016\n",
      "10.000000000000016\n",
      "Yes, it is! \n"
     ]
    }
   ],
   "source": [
    "print (\"To check, let's create a vector v, which when multiplied by H should return a non-zero vector, \\\n",
    "if the point is a minimum\")\n",
    "print (\"T(v) * H * v >0 \")\n",
    "v = np.array([1, 2])\n",
    "\n",
    "print (np.dot(np.dot(v,HessSol),v))\n",
    "print (np.dot(np.dot(v,HessSol),np.transpose(v)))\n",
    "print (\"Yes, it is! \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
